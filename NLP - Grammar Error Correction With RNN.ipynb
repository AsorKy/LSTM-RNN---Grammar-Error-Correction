{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 1)-Importing Dependencies <a class=\"anchor\" id=\"1-bullet\"></a> ","metadata":{}},{"cell_type":"code","source":"!pip install utils","metadata":{"execution":{"iopub.status.busy":"2023-07-25T21:09:12.870273Z","iopub.execute_input":"2023-07-25T21:09:12.870743Z","iopub.status.idle":"2023-07-25T21:09:25.489492Z","shell.execute_reply.started":"2023-07-25T21:09:12.870701Z","shell.execute_reply":"2023-07-25T21:09:25.488119Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Data handling and traditional algebraic operations\nimport math\nimport numpy as np\nimport pandas as pd\nimport random\nimport sys\n\n# DL dependencies\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.utils.data import Dataset as TorchDataset\nfrom torch.utils.tensorboard import SummaryWriter\n\nimport nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk.translate.bleu_score import sentence_bleu, corpus_bleu\nfrom nltk import pos_tag\nnltk.download('punkt')\n\n# ML dependencies and scores\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score, roc_auc_score\nfrom torchtext.data.metrics import bleu_score\n\n# Text manipulation tools\nimport re\nimport string\nimport torchtext\nfrom torchtext.data.utils import get_tokenizer\nfrom torchtext.vocab import build_vocab_from_iterator\n\n# Visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns","metadata":{"id":"r7DbLCy5jJsm","executionInfo":{"status":"ok","timestamp":1690152478898,"user_tz":300,"elapsed":16524,"user":{"displayName":"Nicolas Castillo Ojeda","userId":"07071882850745015721"}},"outputId":"603dd158-9690-40dc-9ab5-c23175cd7fca","execution":{"iopub.status.busy":"2023-07-26T05:01:19.073971Z","iopub.execute_input":"2023-07-26T05:01:19.075109Z","iopub.status.idle":"2023-07-26T05:01:19.084822Z","shell.execute_reply.started":"2023-07-26T05:01:19.075068Z","shell.execute_reply":"2023-07-26T05:01:19.083703Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Device configuration\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","metadata":{"id":"i5HivPJ0pXBU","executionInfo":{"status":"ok","timestamp":1690152480291,"user_tz":300,"elapsed":3,"user":{"displayName":"Nicolas Castillo Ojeda","userId":"07071882850745015721"}},"execution":{"iopub.status.busy":"2023-07-26T04:54:48.566979Z","iopub.execute_input":"2023-07-26T04:54:48.567738Z","iopub.status.idle":"2023-07-26T04:54:48.620209Z","shell.execute_reply.started":"2023-07-26T04:54:48.567703Z","shell.execute_reply":"2023-07-26T04:54:48.619087Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if torch.cuda.is_available():\n    print(\"CUDA is available.\")\nelse:\n    print(\"CUDA is not available.\")","metadata":{"execution":{"iopub.status.busy":"2023-07-26T04:54:51.159954Z","iopub.execute_input":"2023-07-26T04:54:51.160726Z","iopub.status.idle":"2023-07-26T04:54:51.170323Z","shell.execute_reply.started":"2023-07-26T04:54:51.160689Z","shell.execute_reply":"2023-07-26T04:54:51.169192Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1.1- Problem statement <a class=\"anchor\" id=\"1-bullet\"></a> ","metadata":{}},{"cell_type":"markdown","source":"The Sequence-to-Sequence (Seq2Seq) problem is a fundamental task in natural language processing (NLP) and machine translation, where the goal is to transform an input sequence into an output sequence of potentially different lengths. This problem is commonly encountered in tasks such as machine translation, summarization, text generation, Grammar Error Correction (GEC) and more. \n\n**Why Encoder-Decoder Models are Useful:**\n\nEncoder-Decoder models are well-suited for the Seq2Seq problem because they provide an effective framework for handling input and output sequences of varying lengths. The key components of the Encoder-Decoder architecture are:\n\n**1)-Encoder:** The Encoder takes the input sequence and compresses it into a fixed-size context vector, also known as the \"thought vector\" or \"latent representation.\" This context vector aims to capture the essential information from the input sequence and serves as the foundation for generating the output.\n\n**2)-Decoder:** The Decoder takes the context vector produced by the Encoder and generates the output sequence one element at a time. It uses the context vector and the previously generated elements of the output sequence (during training) to conditionally generate the next element in the sequence.\n\nUsing an Encoder-Decoder architecture allows the model to handle variable-length input and output sequences in a way that traditional models like bag-of-words or fixed-size input models cannot.","metadata":{}},{"cell_type":"markdown","source":"**Carrying out the Problem with RNN, LSTM, and LSTM Seq2Seq:**\n\n**1)-RNN(Recurrent Neural Network):**\n* RNNs can be used for Seq2Seq problems by feeding the input sequence step-by-step into the RNN cell and using the final hidden state as the context vector.\n* *Limitation:* RNNs suffer from the vanishing gradient problem, which makes it difficult for them to capture long-range dependencies in sequences, leading to difficulties in handling long sequences.\n\n**2)-LSTM (Long Short-Term Memory):**\n* LSTMs are a variant of RNNs that mitigate the vanishing gradient problem by using memory cells and gating mechanisms.\n* LSTMs can better capture long-term dependencies, making them more effective for Seq2Seq problems compared to simple RNNs.\n\n**3)-LSTM Seq2Seq:**\n* An LSTM-based Seq2Seq model combines two blocks of LSTM networks as both the Encoder and Decoder.\n* The Encoder LSTM processes the input sequence, and the final hidden state becomes the context vector.\n* The Decoder LSTM generates the output sequence by taking the context vector as input and predicting each element of the output sequence step-by-step.\n* *Limitation:* While LSTM Seq2Seq models are an improvement over simple RNNs, they may encounter difficulties with extremely long sequences due to the limitations of LSTMs.\n\nEncoder-Decoder models provide an elegant solution to Seq2Seq problems, enabling the handling of variable-length input and output sequences. RNNs and LSTMs are foundational components for these models, with LSTMs being preferred due to their ability to capture long-range dependencies. However, even LSTM-based Seq2Seq models have limitations in handling very long sequences, and for extremely challenging cases, more advanced architectures like Transformer-based models have been introduced to overcome these limitations.","metadata":{}},{"cell_type":"markdown","source":"## 1.2- Utilities <a class=\"anchor\" id=\"1-bullet\"></a> ","metadata":{}},{"cell_type":"code","source":"def correct_sentence(model, sentence, inp_vocabulary, out_vocabulary, device, max_length=50):\n    # Ensure the sentence is a string and convert tokens to lowercase\n    if not isinstance(sentence, str):\n        raise ValueError(\"Input sentence must be a string\")\n    sentence = sentence.lower()\n\n    # Load tokenizer for English text\n    tokenizer_eng = get_tokenizer('basic_english')\n\n    # Tokenize English sentence\n    tokens = tokenizer_eng(sentence)\n\n    # Add <SOS> and <EOS> tokens in the beginning and end, respectively\n    tokens.insert(0, inp_vocabulary.init_token)\n    tokens.append(inp_vocabulary.eos_token)\n\n    # Convert tokens to indices\n    text_to_indices = [inp_vocabulary.vocab.stoi[token] for token in tokens]\n\n    # Convert to Tensor\n    sentence_tensor = torch.LongTensor(text_to_indices).unsqueeze(1).to(device)\n\n    # Build encoder hidden, cell state\n    with torch.no_grad():\n        hidden, cell = model.encoder(sentence_tensor)\n\n    outputs = [out_vocabulary.vocab.stoi[\"<sos>\"]]\n\n    for _ in range(max_length):\n        previous_word = torch.LongTensor([outputs[-1]]).to(device)\n\n        with torch.no_grad():\n            output, hidden, cell = model.decoder(previous_word, hidden, cell)\n            best_guess = output.argmax(1).item()\n\n        outputs.append(best_guess)\n\n        # Model predicts it's the end of the sentence\n        if output.argmax(1).item() == out_vocabulary.vocab.stoi[\"<eos>\"]:\n            break\n\n    translated_sentence = [out_vocabulary.vocab.itos[idx] for idx in outputs]\n\n    # Remove start token\n    return translated_sentence[1:]\n\n    \n    ","metadata":{"execution":{"iopub.status.busy":"2023-07-26T04:54:54.913112Z","iopub.execute_input":"2023-07-26T04:54:54.913529Z","iopub.status.idle":"2023-07-26T04:54:54.925153Z","shell.execute_reply.started":"2023-07-26T04:54:54.913485Z","shell.execute_reply":"2023-07-26T04:54:54.923926Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def bleu(data, model, input_text, corr_text, device):\n    targets = []\n    outputs = []\n    \n    for example in data:\n        src = vars(example)[\"src\"]\n        trg = vars(example)[\"trg\"]\n        \n        prediction = correct_sentence(model, src, input_text, corr_text, device)\n        prediction = prediction[:1] # remove <eos> token\n        \n        targets.append([trg])\n        outputs.append(prediction)\n    \n    return bleu_score(outputs, targets)","metadata":{"execution":{"iopub.status.busy":"2023-07-26T04:54:56.828081Z","iopub.execute_input":"2023-07-26T04:54:56.828486Z","iopub.status.idle":"2023-07-26T04:54:56.837013Z","shell.execute_reply.started":"2023-07-26T04:54:56.828454Z","shell.execute_reply":"2023-07-26T04:54:56.835333Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def save_checkpoint(state, filename=\"my_checkpoint.pth.tar\"):\n    print(\"=> Saving checkpoint\")\n    torch.save(state, filename)\n    \ndef load_checkpoint(checkpoint, model, optimizer):\n    print(\"=> Loading checkpoint\")\n    model.load_state_dict(checkpoint[\"state_dict\"])\n    optimizer.load_state_dict(checkpoint[\"optimizer\"])","metadata":{"execution":{"iopub.status.busy":"2023-07-26T04:55:00.584051Z","iopub.execute_input":"2023-07-26T04:55:00.584446Z","iopub.status.idle":"2023-07-26T04:55:00.590664Z","shell.execute_reply.started":"2023-07-26T04:55:00.584416Z","shell.execute_reply":"2023-07-26T04:55:00.589608Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2)-Data Preprocessing <a class=\"anchor\" id=\"1-bullet\"></a> \n","metadata":{"id":"YBC0fkFEklIQ"}},{"cell_type":"markdown","source":"## 2.1-Data importing <a class=\"anchor\" id=\"1-bullet\"></a> ","metadata":{"id":"oYfmPht7qZgQ"}},{"cell_type":"markdown","source":"The basic idea is to train the model to take a sentence with grammar errors as input and generate the corrected sentence as the output. However, there are some key considerations to keep in mind when adapting the model for this new task:\n1. The dataset must consists of pairs of sentences where one sentece contains grammar error, and the other sentence, is the same sentence with the errors corrected.\n2. Tokenization and Vocabulary: Ensure that both the input and output sentences are properly tokenized and that you build separate vocabularies for the input and output languages (in this case, the original sentence with errors and the corrected sentence).\n3. Loss Function: Consider using a loss function that is tailored for sequence generation tasks, such as the CrossEntropyLoss, but it should be designed to handle variable- lenght sequences.\n4. Data Augmentation: In grammar error correction, you might not have a huge amount of labeled data. Data augmentation techniques like adding synthetic errors to the correct sentences can help improve the model's generalization and performance.\n5. Preprocessing: Depending on the complexity of the grammar errors you are dealing with, you might need to perform additional preprocessing steps to handle specific error patterns. For example, if you're dealing with spelling mistakes, you might need to use techniques like lemmatization or stemming to handle word variations.\n6. Encoder-Decoder Architecture is the best option to carry sequence to sequence tasks.\n7. Post-processing: After the Seq2Seq model generates the corrected sentence, you might need to perform some post-processing to ensure that the output is in a grammatically correct and coherent form. This can include tasks like capitalization, punctuation, and word order adjustments.\n8. Evaluation: You will need to establish appropriate evaluation metrics for grammar error correction, such as precision, recall, F1 score, or BLEU score, depending on your specific requirements.\n9. Fine-tuning and Regularization: Fine-tuning the pre-trained Seq2Seq model on the grammar error correction task might yield better results. Additionally, regularization techniques like dropout can help prevent overfitting and improve generalization.","metadata":{}},{"cell_type":"markdown","source":"First, we are going to use two standard datasets for grammatical error detection:\n\n*   Lang-8\n\n","metadata":{"id":"S-lGuWfVqeEl"}},{"cell_type":"markdown","source":"### Lang-8 <a class=\"anchor\" id=\"1-bullet\"></a> ","metadata":{"id":"add1r7WZuuLT"}},{"cell_type":"code","source":"# Lang-8 loading and extraction of correct and incorrect sentences\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n#path = \"/kaggle/working/Lang8_entries.train\"\npath = \"/kaggle/input/entriestrain/entries.train\"\n\nf1 = open(path)\nlines1 = f1.readlines()\ninp1 = [] # list for incorrect sentences\ntgt1 = [] # storing\n\nfor i in lines1:\n    lst = i.split(\"\\t\")\n\n# IF LENGTH OF THE LIST IS GREATER THAN 5 THEN CORRECT SENTTENCE EXISTS OTHERWISE ONLY INCORRECT SENTENCE IS PRESENT\n    if len(lst)>5  :     #IF LENGTH IS GREATER THAN 5\n        inp1.append(lst[-2]) # APPEND SECONG LAST ITEM IN LIST WHICH IS INCORRECT SENTENCE\n        tgt1.append(lst[-1]) # APPEND LAST ITEM IN THE LIST WHICH IS","metadata":{"id":"InCGif21rLUn","executionInfo":{"status":"ok","timestamp":1690152493547,"user_tz":300,"elapsed":1904,"user":{"displayName":"Nicolas Castillo Ojeda","userId":"07071882850745015721"}},"execution":{"iopub.status.busy":"2023-07-26T04:56:11.961713Z","iopub.execute_input":"2023-07-26T04:56:11.962121Z","iopub.status.idle":"2023-07-26T04:56:13.439846Z","shell.execute_reply.started":"2023-07-26T04:56:11.962087Z","shell.execute_reply":"2023-07-26T04:56:13.438807Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.DataFrame()\ndf['y'] = list('1'*len(inp1))\ndf['input'] = inp1\ndf['output'] = tgt1","metadata":{"id":"R2LzGW4nttTM","executionInfo":{"status":"ok","timestamp":1690152493830,"user_tz":300,"elapsed":286,"user":{"displayName":"Nicolas Castillo Ojeda","userId":"07071882850745015721"}},"execution":{"iopub.status.busy":"2023-07-26T04:56:15.400116Z","iopub.execute_input":"2023-07-26T04:56:15.400486Z","iopub.status.idle":"2023-07-26T04:56:15.648613Z","shell.execute_reply.started":"2023-07-26T04:56:15.400455Z","shell.execute_reply":"2023-07-26T04:56:15.647542Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"id":"Jgy7GGpqujEr","executionInfo":{"status":"ok","timestamp":1690152493831,"user_tz":300,"elapsed":6,"user":{"displayName":"Nicolas Castillo Ojeda","userId":"07071882850745015721"}},"outputId":"1f1d2453-c13a-4f33-9a2f-fc1f0e6c7a17","execution":{"iopub.status.busy":"2023-07-26T04:07:03.001705Z","iopub.execute_input":"2023-07-26T04:07:03.002427Z","iopub.status.idle":"2023-07-26T04:07:03.012412Z","shell.execute_reply.started":"2023-07-26T04:07:03.002392Z","shell.execute_reply":"2023-07-26T04:07:03.011324Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.describe()","metadata":{"id":"NfP6c4KOyvDB","executionInfo":{"status":"ok","timestamp":1690152495240,"user_tz":300,"elapsed":1414,"user":{"displayName":"Nicolas Castillo Ojeda","userId":"07071882850745015721"}},"outputId":"ae402cbb-562f-46bc-84b7-72b0bd2a9506","execution":{"iopub.status.busy":"2023-07-26T04:02:23.903273Z","iopub.execute_input":"2023-07-26T04:02:23.904016Z","iopub.status.idle":"2023-07-26T04:02:25.027605Z","shell.execute_reply.started":"2023-07-26T04:02:23.903978Z","shell.execute_reply":"2023-07-26T04:02:25.026427Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2.2-Data cleaning <a class=\"anchor\" id=\"1-bullet\"></a> ","metadata":{"id":"ZeLNM2mEuOHM"}},{"cell_type":"markdown","source":"### 2.2.1-Data formating <a class=\"anchor\" id=\"1-bullet\"></a> ","metadata":{"id":"_9k-VnZe1XpT"}},{"cell_type":"code","source":"def remove_spaces(text):\n    text = re.sub(r\" '(\\w)\",r\"'\\1\",text)\n    text = re.sub(r\" \\,\",\",\",text)\n    text = re.sub(r\" \\.+\",\".\",text)\n    text = re.sub(r\" \\!+\",\"!\",text)\n    text = re.sub(r\" \\?+\",\"?\",text)\n    text = re.sub(\" n't\",\"n't\",text)\n    text = re.sub(\"[\\(\\)\\;\\_\\^\\`\\/]\",\"\",text)\n    return text\n\ndef decontract(text):\n    text = re.sub(r\"won\\'t\", \"will not\", text)\n    text = re.sub(r\"can\\'t\", \"can not\", text)\n    text = re.sub(r\"n\\'t\", \" not\", text)\n    text = re.sub(r\"\\'re\", \" are\", text)\n    text = re.sub(r\"\\'s\", \" is\", text)\n    text = re.sub(r\"\\'d\", \" would\", text)\n    text = re.sub(r\"\\'ll\", \" will\", text)\n    text = re.sub(r\"\\'t\", \" not\", text)\n    text = re.sub(r\"\\'ve\", \" have\", text)\n    text = re.sub(r\"\\'m\", \" am\", text)\n    return text\n\ndef preprocess(text):\n    text = re.sub(\"\\n\",\"\",text)\n    text = remove_spaces(text)\n    text = re.sub(r\"\\.+\",\".\",text)\n    text = re.sub(r\"\\!+\",\"!\",text)\n    text = decontract(text)\n    text = re.sub(\"[^A-Za-z0-9 ]+\",\"\",text)\n    text = text.lower()\n    return text","metadata":{"id":"CaNsvTw0ko0B","executionInfo":{"status":"ok","timestamp":1690152498605,"user_tz":300,"elapsed":196,"user":{"displayName":"Nicolas Castillo Ojeda","userId":"07071882850745015721"}},"execution":{"iopub.status.busy":"2023-07-26T04:56:19.911439Z","iopub.execute_input":"2023-07-26T04:56:19.911825Z","iopub.status.idle":"2023-07-26T04:56:19.922641Z","shell.execute_reply.started":"2023-07-26T04:56:19.911792Z","shell.execute_reply":"2023-07-26T04:56:19.921529Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[\"enc_input\"] = df.input.apply(preprocess)\ndf[\"dec_input\"] = df.input.apply(preprocess)\ndf[\"dec_output\"] = df.output.apply(preprocess)\ndf =df.drop([\"input\",\"output\"],axis=1)\ndf = df[df.enc_input.notnull()]\ndf = df[df.dec_input.notnull()]\ndf = df[df.dec_output.notnull()]\ndf = df.drop_duplicates()","metadata":{"id":"C2DVI6SvzgBb","executionInfo":{"status":"ok","timestamp":1690152552791,"user_tz":300,"elapsed":53878,"user":{"displayName":"Nicolas Castillo Ojeda","userId":"07071882850745015721"}},"execution":{"iopub.status.busy":"2023-07-26T04:56:22.715297Z","iopub.execute_input":"2023-07-26T04:56:22.715676Z","iopub.status.idle":"2023-07-26T04:57:26.080291Z","shell.execute_reply.started":"2023-07-26T04:56:22.715644Z","shell.execute_reply":"2023-07-26T04:57:26.079145Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"id":"LV6NmU8wz9ny","executionInfo":{"status":"ok","timestamp":1690152552793,"user_tz":300,"elapsed":12,"user":{"displayName":"Nicolas Castillo Ojeda","userId":"07071882850745015721"}},"outputId":"07670fd8-cb0d-4e6c-f4d5-e897a66e9ab1","execution":{"iopub.status.busy":"2023-07-26T04:08:09.554547Z","iopub.execute_input":"2023-07-26T04:08:09.554936Z","iopub.status.idle":"2023-07-26T04:08:09.568747Z","shell.execute_reply.started":"2023-07-26T04:08:09.554903Z","shell.execute_reply":"2023-07-26T04:08:09.567494Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.2.2-Removing null values and duplicates <a class=\"anchor\" id=\"1-bullet\"></a> ","metadata":{"id":"ZKOWSGOZ1b2B"}},{"cell_type":"code","source":"df.shape","metadata":{"id":"K8BjFSoy1QYi","executionInfo":{"status":"ok","timestamp":1690152651251,"user_tz":300,"elapsed":240,"user":{"displayName":"Nicolas Castillo Ojeda","userId":"07071882850745015721"}},"outputId":"60c9a24c-336a-4d77-9c3c-9de6a47a295c","execution":{"iopub.status.busy":"2023-07-26T04:05:44.736784Z","iopub.execute_input":"2023-07-26T04:05:44.737306Z","iopub.status.idle":"2023-07-26T04:05:44.749777Z","shell.execute_reply.started":"2023-07-26T04:05:44.737252Z","shell.execute_reply":"2023-07-26T04:05:44.748785Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = df.drop_duplicates()\ndf = df[df.enc_input.notnull()]\ndf = df[df.dec_input.notnull()]\ndf = df[df.dec_output.notnull()]\nprint(df.shape)\ndf.head()","metadata":{"id":"zdDjW0wE1jQa","executionInfo":{"status":"ok","timestamp":1690152652386,"user_tz":300,"elapsed":820,"user":{"displayName":"Nicolas Castillo Ojeda","userId":"07071882850745015721"}},"outputId":"c281a6ca-553e-4ea6-ba69-32e4f9c0c686","execution":{"iopub.status.busy":"2023-07-26T04:57:38.313399Z","iopub.execute_input":"2023-07-26T04:57:38.313766Z","iopub.status.idle":"2023-07-26T04:57:39.339972Z","shell.execute_reply.started":"2023-07-26T04:57:38.313734Z","shell.execute_reply":"2023-07-26T04:57:39.338859Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2.2-Tokenization <a class=\"anchor\" id=\"1-bullet\"></a> ","metadata":{"id":"uXVqj0eclBOm"}},{"cell_type":"markdown","source":"We crate a tokenizer that takes into account the special ``<start>`` and ``<end>`` tokens. So we first add these tokens to the data to ensure correct formating.","metadata":{"id":"8eAOighCgyLU"}},{"cell_type":"code","source":"# Add <start> and <end> tokens to the dec_input column\n# df[\"dec_input\"] = df[\"dec_input\"].apply(lambda x: \"<start> \" + x + \" <end>\")\ndf[\"dec_input\"] = df[\"dec_input\"].apply(lambda x: \"<sos> \" + x)\n\n# Add <start> and <end> tokens to the dec_output column\n#df[\"dec_output\"] = df[\"dec_output\"].apply(lambda x: \"<start> \" + x + \" <end>\")\ndf[\"dec_output\"] = df[\"dec_output\"].apply(lambda x: x + \" <eos>\")\ndf.head()","metadata":{"id":"OaGM7VvF1SPA","executionInfo":{"status":"ok","timestamp":1690152654202,"user_tz":300,"elapsed":727,"user":{"displayName":"Nicolas Castillo Ojeda","userId":"07071882850745015721"}},"outputId":"9ed28140-66b9-42b0-defa-08382fee2c7d","execution":{"iopub.status.busy":"2023-07-26T04:57:43.052258Z","iopub.execute_input":"2023-07-26T04:57:43.052804Z","iopub.status.idle":"2023-07-26T04:57:43.744695Z","shell.execute_reply.started":"2023-07-26T04:57:43.052763Z","shell.execute_reply":"2023-07-26T04:57:43.743749Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(df[\"dec_input\"].iloc[1])\nprint(df[\"dec_output\"].iloc[1])","metadata":{"id":"YrqcBrKs1q4Q","executionInfo":{"status":"ok","timestamp":1690152658932,"user_tz":300,"elapsed":219,"user":{"displayName":"Nicolas Castillo Ojeda","userId":"07071882850745015721"}},"outputId":"de0def77-340a-4492-f0d6-bd8a1ebb8e76","execution":{"iopub.status.busy":"2023-07-26T04:06:07.460690Z","iopub.execute_input":"2023-07-26T04:06:07.461378Z","iopub.status.idle":"2023-07-26T04:06:07.467352Z","shell.execute_reply.started":"2023-07-26T04:06:07.461342Z","shell.execute_reply":"2023-07-26T04:06:07.466296Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# DONÂ´T RUN THIS CELL\n\n# Adding <start> and <end> token\ndf[\"dec_input\"] = \"<start>\"+ df[\"dec_input\"]\ndf[\"dec_input\"].iloc[0] = df[\"dec_input\"].iloc[0] + \"<end>\"\ndf[\"dec_output\"].iloc[0] = df[\"dec_output\"].iloc[0] + \"<end>\"\ndf.head()","metadata":{"id":"SzeD2dCbVOY_","executionInfo":{"status":"ok","timestamp":1689948194798,"user_tz":300,"elapsed":540,"user":{"displayName":"Nicolas Castillo Ojeda","userId":"07071882850745015721"}},"outputId":"2d83685b-821b-412d-9057-72e1cf7c5be6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We then split the original dataset into training, validation and test sets.","metadata":{"id":"K5ExIWE2hAs7"}},{"cell_type":"code","source":"# Train, validation and test split\ndf_train ,df_val_train = train_test_split(df, test_size=0.3,random_state = 16, stratify = df.y )\ndf_val, df_test = train_test_split(df_val_train, test_size=0.5, random_state = 16, stratify = df_val_train.y)","metadata":{"id":"CDu0MdP3TyUY","executionInfo":{"status":"ok","timestamp":1690152663559,"user_tz":300,"elapsed":1563,"user":{"displayName":"Nicolas Castillo Ojeda","userId":"07071882850745015721"}},"execution":{"iopub.status.busy":"2023-07-26T04:57:51.869936Z","iopub.execute_input":"2023-07-26T04:57:51.870667Z","iopub.status.idle":"2023-07-26T04:57:53.129985Z","shell.execute_reply.started":"2023-07-26T04:57:51.870634Z","shell.execute_reply":"2023-07-26T04:57:53.128956Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Train Shape =\",df_train.shape)\nprint(\"Val Shape =\",df_val.shape)\nprint(\"Test Shape =\",df_test.shape)","metadata":{"id":"U5vyoojyYoAX","executionInfo":{"status":"ok","timestamp":1690145181477,"user_tz":300,"elapsed":3,"user":{"displayName":"Nicolas Castillo Ojeda","userId":"07071882850745015721"}},"outputId":"4ee0737b-ba43-4d27-db4d-3dd55236f2fe","execution":{"iopub.status.busy":"2023-07-26T04:08:37.494021Z","iopub.execute_input":"2023-07-26T04:08:37.494395Z","iopub.status.idle":"2023-07-26T04:08:37.502763Z","shell.execute_reply.started":"2023-07-26T04:08:37.494364Z","shell.execute_reply":"2023-07-26T04:08:37.501397Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Finally, we run the tokenizer using ```get_tokenizer``, perform a word counter for future uses using the ``Counter()`` cosntructor and generate the input and output training vocabulary for the encoder using ```build_vocab_from_iterator``.","metadata":{"id":"WYS3ZKK7hcTs"}},{"cell_type":"code","source":"from collections import Counter\n\ndef count_token_occurrences(tokens_list):\n  counter = Counter()\n  for sentence_tokens in tokens_list:\n    counter.update(sentence_tokens)\n  return counter\n\n# Define the tokenizer function (use basic_english tokenizer)\ntokenizer = get_tokenizer('basic_english')\ntraindata_in = df_train.dec_input.apply(str).tolist()\ntraindata_out = df_train.dec_output.apply(str).tolist()\n\n# Tokenization and preprocessing for encoder input\nenc_input_tokens = [tokenizer(sentence) for sentence in traindata_in]\n\n# Tokenization and preprocessing for decoder input\ndec_input_tokens = [tokenizer(sentence) for sentence in traindata_out]\n\n# Build vocabulary for encoder input\ncounter_enc = count_token_occurrences(enc_input_tokens)\ntk_inp = build_vocab_from_iterator(enc_input_tokens,specials=['<pad>'])\n# Build vocabulary for decoder input\ncounter_dec = count_token_occurrences(dec_input_tokens)\ntk_out = build_vocab_from_iterator(dec_input_tokens, specials=['<pad>', '<sos>', '<eos>'])\n","metadata":{"id":"q8yBcJ-_g930","executionInfo":{"status":"ok","timestamp":1690152680963,"user_tz":300,"elapsed":16277,"user":{"displayName":"Nicolas Castillo Ojeda","userId":"07071882850745015721"}},"execution":{"iopub.status.busy":"2023-07-26T04:58:06.081546Z","iopub.execute_input":"2023-07-26T04:58:06.082113Z","iopub.status.idle":"2023-07-26T04:58:22.862478Z","shell.execute_reply.started":"2023-07-26T04:58:06.082073Z","shell.execute_reply":"2023-07-26T04:58:22.861414Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(type(tk_out.get_stoi()))","metadata":{"id":"P298VrbjjHYD","executionInfo":{"status":"ok","timestamp":1690061022100,"user_tz":300,"elapsed":221,"user":{"displayName":"Nicolas Castillo Ojeda","userId":"07071882850745015721"}},"outputId":"4de889da-8e46-4c7e-bdf2-93918e8b97ba","execution":{"iopub.status.busy":"2023-07-26T04:58:39.524906Z","iopub.execute_input":"2023-07-26T04:58:39.525984Z","iopub.status.idle":"2023-07-26T04:58:39.570267Z","shell.execute_reply.started":"2023-07-26T04:58:39.525934Z","shell.execute_reply":"2023-07-26T04:58:39.569146Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3.3-Text data into integer sequences <a class=\"anchor\" id=\"1-bullet\"></a> ","metadata":{"id":"QVo3C5aljh7j"}},{"cell_type":"markdown","source":"We now try to convert the text data into integer sequences wich also has a padding. This padding of sequences is necessary to ensure that all sequences in a batch have the same length. Padding adds special tokens (pad token) to the sequences to that all sequences pocesses the same amount of tokens.","metadata":{"id":"zjyHBHC9qKsJ"}},{"cell_type":"code","source":"class conv_dataset(TorchDataset):\n    def __init__(self, data, tk_inp, tk_out, max_len):\n        self.encoder_in = data[\"enc_input\"].apply(str).values\n        self.decoder_in = data[\"dec_input\"].apply(str).values\n        self.decoder_out = data[\"dec_output\"].apply(str).values\n        self.tk_inp = tk_inp.get_stoi()\n        self.tk_out = tk_out.get_stoi()\n        self.tokenizer = get_tokenizer('basic_english')\n        self.max_len = max_len\n\n    def __getitem__(self, i):\n        # Input sequences\n        encoder_seq = self.encoder_in[i]\n        encoder_tokens = self.tokenizer(encoder_seq)\n        encoder_indices = [self.tk_inp[token] for token in encoder_tokens]\n        encoder_tensor = torch.tensor(encoder_indices)\n\n        # Input encoder sequences\n        decoder_in_seq = self.decoder_in[i]\n        decoder_in_tokens = self.tokenizer(decoder_in_seq)\n        # Special handling for <start> and <end> tokens\n        decoder_in_indices = [self.tk_inp[token] for token in decoder_in_tokens]\n        decoder_in_tensor = torch.tensor(decoder_in_indices)\n\n        # Input decoder sequences\n        decoder_out_seq = self.decoder_out[i]\n        decoder_tokens = self.tokenizer(decoder_out_seq)\n        # Special handling for <start> and <end> tokens\n        decoder_out_indices = [self.tk_out[token] for token in decoder_tokens]\n        decoder_out_tensor = torch.tensor(decoder_out_indices)\n\n        # Tokenizer padding\n        encoder_tensor = F.pad(encoder_tensor, pad=(0, self.max_len - len(encoder_tensor)))\n        decoder_in_tensor = F.pad(decoder_in_tensor, pad=(0, self.max_len - len(decoder_in_tensor)))\n        decoder_out_tensor = F.pad(decoder_out_tensor, pad=(0, self.max_len - len(decoder_out_tensor)))\n\n        return encoder_tensor, decoder_in_tensor, decoder_out_tensor\n\n    def __len__(self):\n        return len(self.encoder_in)","metadata":{"id":"yJb1TRf68y9O","executionInfo":{"status":"ok","timestamp":1690152701432,"user_tz":300,"elapsed":224,"user":{"displayName":"Nicolas Castillo Ojeda","userId":"07071882850745015721"}},"execution":{"iopub.status.busy":"2023-07-26T04:58:42.971341Z","iopub.execute_input":"2023-07-26T04:58:42.971718Z","iopub.status.idle":"2023-07-26T04:58:42.984144Z","shell.execute_reply.started":"2023-07-26T04:58:42.971688Z","shell.execute_reply":"2023-07-26T04:58:42.982885Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3.4-Data batching <a class=\"anchor\" id=\"1-bullet\"></a> ","metadata":{"id":"qgcyupcugeff"}},{"cell_type":"markdown","source":"##### 3.4.1-DataLoader from scratch <a class=\"anchor\" id=\"1-bullet\"></a> ","metadata":{"id":"WVPXKrZPgiBm"}},{"cell_type":"code","source":"class Dataloader(DataLoader):\n  def __init__(self, batch_size, dataset):\n    self.dataset = dataset\n    self.batch_size = batch_size\n    self.total_points = self.dataset.encoder_in.shape[0]\n\n  def __iter__(self):\n    for i in range(len(self)):\n      yield self.dataset[i]\n\n  def __getitem__(self,i):\n    start = i * self.batch_size\n    stop = (i+1) * self.batch_size\n\n    batch_enc = []\n    batch_dec_input = []\n    batch_dec_out = []\n\n    for j in range(start, stop):\n      a, b, c = self.dataset[j]\n      batch_enc.append(a[0])\n      batch_dec_input.append(b[0])\n      batch_dec_out.append(c[0])\n\n    batch_enc = torch.tensor(batch_enc, dtype=torch.long)\n    batch_dec_input = torch.tensor(batch_dec_input, dtype=torch.long)\n    batch_dec_out = torch.tensor(batch_dec_out, dtype=torch.long)\n\n    return [batch_enc, batch_dec_input], batch_dec_out\n\n  def __len__(self):\n    #return math.ceil(self.total_points / self.batch_size)\n    return math.ceil(len(self.dataset) / self.batch_size)","metadata":{"id":"rjyMeqgG2tdw","execution":{"iopub.status.busy":"2023-07-25T14:17:11.644347Z","iopub.execute_input":"2023-07-25T14:17:11.645261Z","iopub.status.idle":"2023-07-25T14:17:11.659288Z","shell.execute_reply.started":"2023-07-25T14:17:11.645223Z","shell.execute_reply":"2023-07-25T14:17:11.658338Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch.utils.data as data\n\nclass Dataloader(DataLoader):\n    def __init__(self, batch_size, dataset, shuffle=True):\n        self.dataset = dataset\n        self.batch_size = batch_size\n        self.shuffle = shuffle\n        self.total_points = self.dataset.encoder_in.shape[0]\n\n    def __iter__(self):\n        if self.shuffle:\n            indices = torch.randperm(len(self.dataset))\n            self.dataset = data.Subset(self.dataset, indices)\n        for i in range(len(self)):\n            yield self.dataset[i]\n\n    def __getitem__(self, i):\n        start = i * self.batch_size\n        stop = (i + 1) * self.batch_size\n\n        batch_enc = []\n        batch_dec_input = []\n        batch_dec_out = []\n\n        for j in range(start, stop):\n            a, b, c = self.dataset[j]  # Modify this line to unpack only encoder and decoder outputs\n            batch_enc.append(a)\n            batch_dec_input.append(b)\n            batch_dec_out.append(c)\n\n        batch_enc = torch.tensor(batch_enc, dtype=torch.long)\n        batch_dec_input = torch.tensor(batch_dec_input, dtype=torch.long)\n        batch_dec_out = torch.tensor(batch_dec_out, dtype=torch.long)\n\n        return batch_enc, batch_dec_input, batch_dec_out   # Return inputs and targets as a tuple\n\n    def __len__(self):\n        return int(self.total_points / self.batch_size)\n","metadata":{"execution":{"iopub.status.busy":"2023-07-25T14:24:05.268179Z","iopub.execute_input":"2023-07-25T14:24:05.268578Z","iopub.status.idle":"2023-07-25T14:24:05.280710Z","shell.execute_reply.started":"2023-07-25T14:24:05.268522Z","shell.execute_reply":"2023-07-25T14:24:05.279600Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### 3.4.2-Built-in DataLoader  <a class=\"anchor\" id=\"1-bullet\"></a> ","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.utils.data as data\n\ndef custom_collate_fn(batch):\n    enc_inputs, dec_inputs, dec_outputs = zip(*batch)\n    enc_inputs = torch.stack(enc_inputs, dim=0).long()  # Use 'long' dtype for the encoder input\n    dec_inputs = torch.stack(dec_inputs, dim=0).long()  # Use 'long' dtype for the decoder input\n    dec_outputs = torch.stack(dec_outputs, dim=0).long()  # Use 'long' dtype for the decoder output\n    return enc_inputs, dec_inputs, dec_outputs\n\ndef custom_collate_fn(batch):\n    enc_inputs, dec_inputs, dec_outputs = zip(*batch)\n\n    # Pad the input sequences to the maximum length in the batch\n    enc_inputs = torch.nn.utils.rnn.pad_sequence(enc_inputs, batch_first=True)\n    dec_inputs = torch.nn.utils.rnn.pad_sequence(dec_inputs, batch_first=True)\n    dec_outputs = torch.nn.utils.rnn.pad_sequence(dec_outputs, batch_first=True)\n\n    return enc_inputs, dec_inputs, dec_outputs\n\n# Create the DataLoader with the custom collate function\n\n# Train processed data\ntrain_data = conv_dataset(df_train, tk_inp, tk_out, 35)\ntrain_loader = DataLoader(batch_size=256, dataset=train_data, shuffle=True, collate_fn=custom_collate_fn)\n\n# Validation processed data\nval_data = conv_dataset(df_val, tk_inp, tk_out, 35)\nval_loader = DataLoader(batch_size=256, dataset=val_data, shuffle=True, collate_fn=custom_collate_fn)\n\n# Test processed data\ntest_data = conv_dataset(df_test, tk_inp, tk_out, 35)\ntest_loader = DataLoader(batch_size=256, dataset=test_data, shuffle=True, collate_fn=custom_collate_fn)\n","metadata":{"execution":{"iopub.status.busy":"2023-07-26T05:11:10.056156Z","iopub.execute_input":"2023-07-26T05:11:10.056528Z","iopub.status.idle":"2023-07-26T05:11:10.674682Z","shell.execute_reply.started":"2023-07-26T05:11:10.056499Z","shell.execute_reply":"2023-07-26T05:11:10.673495Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train processed data\ntrain_data = conv_dataset(df_train, tk_inp, tk_out, 35)\ntrain_loader = DataLoader(batch_size=8, dataset=train_data, shuffle=True)\n\n# Validation processed data\nval_data = conv_dataset(df_val, tk_inp, tk_out, 35)\nval_loader = Dataloader(batch_size=8, dataset=val_data, shuffle=True)\n\n# Test processed data\ntest_data = conv_dataset(df_test, tk_inp, tk_out, 35)\ntest_loader = Dataloader(batch_size=8, dataset=test_data, shuffle=True)","metadata":{"id":"szyhh_PX0qV_","executionInfo":{"status":"ok","timestamp":1690152726462,"user_tz":300,"elapsed":1335,"user":{"displayName":"Nicolas Castillo Ojeda","userId":"07071882850745015721"}},"execution":{"iopub.status.busy":"2023-07-25T15:16:23.353261Z","iopub.execute_input":"2023-07-25T15:16:23.353653Z","iopub.status.idle":"2023-07-25T15:16:23.956760Z","shell.execute_reply.started":"2023-07-25T15:16:23.353622Z","shell.execute_reply":"2023-07-25T15:16:23.955695Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_DL = train_loader\ndata_iter = iter(train_DL)\nbatch_data = next(data_iter)\n\nfor tensor in batch_data:\n    print(\"Tensor shape:\", tensor.shape)","metadata":{"id":"LP-kc9Dv3w7v","executionInfo":{"status":"ok","timestamp":1690151686081,"user_tz":300,"elapsed":411,"user":{"displayName":"Nicolas Castillo Ojeda","userId":"07071882850745015721"}},"outputId":"75b8ad22-f354-406d-bd86-ee8afd0a0b35","execution":{"iopub.status.busy":"2023-07-26T04:09:44.750564Z","iopub.execute_input":"2023-07-26T04:09:44.750958Z","iopub.status.idle":"2023-07-26T04:09:44.883561Z","shell.execute_reply.started":"2023-07-26T04:09:44.750927Z","shell.execute_reply":"2023-07-26T04:09:44.882575Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch_size = 512\n\n# Train processed data\ntrain_Dataloader = torch.utils.data.DataLoader(dataset=train_data,\n                                               batch_size=batch_size,\n                                               shuffle=True)\n\n# Validation processed data\nval_Dataloader = torch.utils.data.DataLoader(dataset=val_data,\n                                               batch_size=batch_size,\n                                               shuffle=True)\n\n# Test processed data\ntest_Dataloader = torch.utils.data.DataLoader(dataset=test_data,\n                                               batch_size=batch_size,\n                                               shuffle=True)","metadata":{"id":"OdGIXIE2g8HO","execution":{"iopub.status.busy":"2023-07-24T00:15:39.508353Z","iopub.execute_input":"2023-07-24T00:15:39.508742Z","iopub.status.idle":"2023-07-24T00:15:39.515853Z","shell.execute_reply.started":"2023-07-24T00:15:39.508710Z","shell.execute_reply":"2023-07-24T00:15:39.514492Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"To check that the vocabulary is correctly generated, we perform a small hand-made tokenisation of a phrase from the dataset and compare its tokens with respecto to the kyes of the dictionary containing the vocabulary, to see if the elements match in apossible assigment made from the PyTorch vocabulary. However, we have to perform a re-organization of the keys that match the values(indices) we are looking for with respect to the sample phrase, because the output of this search is not an organsed list.","metadata":{"id":"Nb5m4NBkK67q"}},{"cell_type":"code","source":"# vocabularies to test\nencoder_vocab = tk_inp.get_stoi()\ndecorer_in_vocab = tk_inp.get_stoi()\ndecoder_out_vocab = tk_out.get_stoi()\nvocabs = [encoder_vocab, decorer_in_vocab, decoder_out_vocab]\n\n# examples to test\nsample_index = 2\nvocab_ref = 2\n\n# manually constructed tokens to test the vocabulary\nthe_phrase = df_train[\"dec_input\"].iloc[sample_index]\nthe_splited_phrase = the_phrase.split()\n\n# manually extracted indices for the sampled phrase\ncheck_list_encoder = [encoder_vocab[word] for word in the_splited_phrase]\ncheck_list_in_decoder = [decorer_in_vocab[word] for word in the_splited_phrase]\ncheck_list_out_decoder = [decoder_out_vocab[word] for word in the_splited_phrase]\n\n# sample from the generated vocabulary\nsample = train_data[sample_index]\n\n# Extract the keys that are present in the big dictionary and also in the values_list\nfiltered_keys = [key for key in vocabs[vocab_ref].keys() if vocabs[vocab_ref][key] in sample[vocab_ref].numpy()]\n\n# Sort the filtered keys based on the order of values_list\nvalues_list = list(sample[vocab_ref].numpy())\nsorted_keys = sorted(filtered_keys, key=lambda x: values_list.index(vocabs[vocab_ref][x]))\n\nprint(\"*\"*60)\nprint(\"Phrase to check =\", the_phrase)\nprint(\"*\"*60)\nprint(\"Phrase according to the encoder_vocab, dec_in_vocab and dec_out_vocab =\")\nprint(\"manual encoder_view = \",check_list_encoder)\nprint(\"manual in_decoder_view = \",check_list_in_decoder)\nprint(\"manual out_decoder_view =\",check_list_out_decoder)\nprint(\"*\"*60)\nprint(\"Reconstructed phrase according to the selected vocabulary\")\nprint(sorted_keys)\nprint(\"*\"*60)\nprint(\"torch tensor numerization of selected phrase\")\nsample","metadata":{"id":"Ys8kiMZu1Vju","executionInfo":{"status":"ok","timestamp":1690151890205,"user_tz":300,"elapsed":1021,"user":{"displayName":"Nicolas Castillo Ojeda","userId":"07071882850745015721"}},"outputId":"cb8e6d76-0738-4b5e-905b-3b1dda83241f","execution":{"iopub.status.busy":"2023-07-26T04:09:52.746355Z","iopub.execute_input":"2023-07-26T04:09:52.746720Z","iopub.status.idle":"2023-07-26T04:09:53.427802Z","shell.execute_reply.started":"2023-07-26T04:09:52.746691Z","shell.execute_reply":"2023-07-26T04:09:53.426738Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"encoder_vocab = tk_inp.get_stoi()\ndecorer_in_vocab = tk_inp.get_stoi()\ndecoder_out_vocab = tk_out.get_stoi()\n\nprint(encoder_vocab['<end>'])\nprint(decorer_in_vocab['<end>'])\nprint(decoder_out_vocab['<end>'])","metadata":{"id":"EH7Wccq15WK1","executionInfo":{"status":"ok","timestamp":1690086643071,"user_tz":300,"elapsed":2,"user":{"displayName":"Nicolas Castillo Ojeda","userId":"07071882850745015721"}},"outputId":"a1ef7c08-c83d-4889-bd81-b8de228a232a","execution":{"iopub.status.busy":"2023-07-25T15:29:18.278477Z","iopub.execute_input":"2023-07-25T15:29:18.279196Z","iopub.status.idle":"2023-07-25T15:29:18.395745Z","shell.execute_reply.started":"2023-07-25T15:29:18.279151Z","shell.execute_reply":"2023-07-25T15:29:18.394332Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3)-Model Architecture <a class=\"anchor\" id=\"1-bullet\"></a> ","metadata":{"id":"yfnAOpiwkpU1"}},{"cell_type":"markdown","source":"1. Training: The model is trained on batches of sequences, where each sequence has a fixed lenght defined during data preprocessing, where the sentences are padded or truncated to the specified lenght. During the forward pass, the input sequences are preprocessed by the RNN/LST/GRN and the output sequences will have the same sequence lenght as the input sequences.\n\n2. Testing (inference): During testing, we can input sequences of varying lenghts to the trained model. However the model will requiere fixed-lenght input sequences to process them in batches. If our input sequence is shorter than the fixed lenght,  we would need to pad it to reach the specified lenght. The output sequence will be of the same lenght as the input sequence because the RNN/LSTM/GRN processes each input token and generates an output token at each time step.","metadata":{"id":"5zynQJuWv8py"}},{"cell_type":"markdown","source":"## 3.1-RNN Model <a class=\"anchor\" id=\"1-bullet\"></a> ","metadata":{"id":"234e96Uch--d"}},{"cell_type":"markdown","source":"### 3.1.1-Architecture <a class=\"anchor\" id=\"1-bullet\"></a> ","metadata":{"id":"SucMeSkGiGhV"}},{"cell_type":"code","source":"class RNN(nn.Module):\n\n  def __init__(self, input_size, embedding_size, hidden_size, num_layers, vocab_size):\n    super(RNN, self).__init__()\n    self.input_size = input_size\n    self.embedding_size = embedding_size\n    self.hidden_size = hidden_size\n    self.num_layers = num_layers\n    self.vocab_size = vocab_size\n\n    # Layers of the model\n    # -> x.shape() = (batch_size, seq, input_size)\n    self.embedding = nn.Embedding(input_size, embedding_size)\n    self.rnn = nn.RNN(embedding_size, hidden_size, num_layers, batch_first=True)\n    self.fc = nn.Linear(hidden_size, vocab_size)\n\n  def forward(self, x):\n    h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n    embeds = self.embedding(x)\n    out, h_out = self.rnn(embeds, h0)\n    out = self.fc(out) # use the entire output for correction\n    return out\n","metadata":{"id":"_YR0GI2uiGKM","executionInfo":{"status":"ok","timestamp":1690152737033,"user_tz":300,"elapsed":225,"user":{"displayName":"Nicolas Castillo Ojeda","userId":"07071882850745015721"}},"execution":{"iopub.status.busy":"2023-07-25T15:29:23.584828Z","iopub.execute_input":"2023-07-25T15:29:23.585223Z","iopub.status.idle":"2023-07-25T15:29:23.593223Z","shell.execute_reply.started":"2023-07-25T15:29:23.585190Z","shell.execute_reply":"2023-07-25T15:29:23.592264Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Model Hipper parameters\nnum_layers = 3\nlearning_rate = 0.001\nnum_epochs = 10\n\n# x hipper parameters\nbatch_size = 256\ninput_size = 35\nsequence_length = 35\noutput_size = 35\nhidden_size = 256 # encoding units\n\nvocab_size = len(tk_inp)\nnum_classes = vocab_size\nembedding_size = 150\n\n# Momentum\nbeta1 = 0.1  # Momentum value for the momentum term in Adam\nbeta2 = 0.1  # Value for the squared gradient term in Adam","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Model RNN instance\nimport torch\ntorch.cuda.empty_cache()\n\nmodel_rnn = RNN(input_size = vocab_size,\n                embedding_size = embedding_size,\n                hidden_size = hidden_size,\n                num_layers = num_layers,\n                vocab_size = vocab_size).to(device)","metadata":{"id":"wJQKe_2ek67E","executionInfo":{"status":"ok","timestamp":1690152740211,"user_tz":300,"elapsed":937,"user":{"displayName":"Nicolas Castillo Ojeda","userId":"07071882850745015721"}},"execution":{"iopub.status.busy":"2023-07-25T15:29:26.042416Z","iopub.execute_input":"2023-07-25T15:29:26.043027Z","iopub.status.idle":"2023-07-25T15:29:26.370262Z","shell.execute_reply.started":"2023-07-25T15:29:26.042993Z","shell.execute_reply":"2023-07-25T15:29:26.369216Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Loss and optimizer\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model_rnn.parameters(),\n                             lr = learning_rate\n                             #,betas=(beta1, beta2)\n                            )\n                        ","metadata":{"id":"5nFfsZVdlN67","executionInfo":{"status":"ok","timestamp":1690152741461,"user_tz":300,"elapsed":235,"user":{"displayName":"Nicolas Castillo Ojeda","userId":"07071882850745015721"}},"execution":{"iopub.status.busy":"2023-07-25T15:29:28.127915Z","iopub.execute_input":"2023-07-25T15:29:28.128923Z","iopub.status.idle":"2023-07-25T15:29:28.135081Z","shell.execute_reply.started":"2023-07-25T15:29:28.128877Z","shell.execute_reply":"2023-07-25T15:29:28.133563Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3.1.2-Training procedure <a class=\"anchor\" id=\"1-bullet\"></a> ","metadata":{"id":"MNayDDzEiLcF"}},{"cell_type":"code","source":"#Training procedure\nimport torch\ntorch.cuda.empty_cache()\n\n\ntrain_DL = train_loader\n\nn_total_steps = len(train_DL)\nloss_history_epochs = []\nloss_history_batches = []\n\nfor epoch in range(num_epochs):\n  total_loss = 0\n  for i, batch_data in enumerate(train_DL):\n\n    enc_input, dec_input, dec_output = batch_data\n    enc_input, dec_input, dec_output = enc_input.to(device), dec_input.to(device), dec_output.to(device)\n    inputs, targets = enc_input, dec_output\n\n    # Forward pass\n    outputs = model_rnn(inputs)\n    # Loss and Backpropagation\n    loss = criterion(outputs.view(-1, vocab_size), targets.view(-1))\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n\n    total_loss += loss.item()\n\n    if (i + 1) % 100 == 0:\n      avg_loss = total_loss / input_size\n      print(f'Epoch [{epoch + 1}/{num_epochs}], Step [{i + 1}/{len(train_loader)}], Loss: {avg_loss:.4f}')\n      total_loss = 0\n      loss_history_batches.append(avg_loss)\n  loss_history_epochs.append(avg_loss)\n","metadata":{"id":"rne-nGAGiFLI","execution":{"iopub.status.busy":"2023-07-25T15:29:30.689454Z","iopub.execute_input":"2023-07-25T15:29:30.690134Z","iopub.status.idle":"2023-07-25T16:25:15.058477Z","shell.execute_reply.started":"2023-07-25T15:29:30.690102Z","shell.execute_reply":"2023-07-25T16:25:15.057395Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3.2-LSTM Models <a class=\"anchor\" id=\"1-bullet\"></a> ","metadata":{"id":"3o2odB2chno_"}},{"cell_type":"markdown","source":"### 3.2.1-Simple LSTM model <a class=\"anchor\" id=\"1-bullet\"></a> ","metadata":{}},{"cell_type":"code","source":"class Simp_LSTM(nn.Module):\n\n  def __init__(self, input_size, embedding_size, hidden_size, num_layers, vocab_size):\n    \"\"\"\n    -> x.shape() = (batch_size, seq, input_size)\n    -> out.shape() = (batch_size, seq_length, hidden_size)\n    -> self.fc(out) = (seq_length)\n    -> h0.shape() = (num_layers, batch_size, hidden_size)\n    -> c0.shape() =\n    \"\"\"\n    \n    super(RNN, self).__init__()\n    self.input_size = input_size\n    self.embedding_size = embedding_size # Input size\n    self.hidden_size = hidden_size # Number of encoder units\n    self.num_layers = num_layers\n    self.vocab_size = vocab_size\n\n    # Layers of the model\n    self.embedding = nn.Embedding(input_size, embedding_size)\n    self.lstm = nn.LSTM(embedding_size, hidden_size, num_layers, batch_first=True)\n    self.fc = nn.Linear(hidden_size, vocab_size)\n\n  def forward(self, x):\n    h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n    c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n    embeds = self.embedding(x)\n    out, h_out = self.lstm(embeds, (h0, c0) ) # out:(batch_size, seq_length, hidden_size)\n    out = self.fc(out) # use the entire output for correction                                                      \n    return out","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Model Simple LSTM instance\nimport torch\ntorch.cuda.empty_cache()\n\nmodel_Simp_LSTM = Simp_LSTM(input_size = vocab_size,\n                embedding_size = embedding_size,\n                hidden_size = hidden_size,\n                num_layers = num_layers,\n                vocab_size = vocab_size).to(device)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3.2.2-Simple seq2seq LSTM model <a class=\"anchor\" id=\"1-bullet\"></a> ","metadata":{}},{"cell_type":"markdown","source":"<img src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6yI-Ecx36JYixgomc-inPg.png\">\n","metadata":{}},{"cell_type":"markdown","source":"<img src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0aHodc667UfSyZj-UY8OQw.png\">","metadata":{}},{"cell_type":"markdown","source":"**Encoder:**\n\n*1)-Input:*\n  * x.shape = (seq_length, N) where N is the batch size\n\n*2)-Embedding Layer:*\n  * embedding.shape = (seq_length, N, embedding_size)\n\n*3)-LSTM Layer:*\n  * outputs.shape = (seq_length, N, hidden_size)\n  * hidden.shape = (num_layers, N, hidden_size)\n  * cell.shape = (num_layers, N, hidden_size)","metadata":{}},{"cell_type":"markdown","source":"**Decoder:**\n\n*1)- Input:*\n  * x.shape = (N), where N is for batch_size, we     want it to be (1,N), because the seq_lenght is 1 here because we are sending in a single word and not a sequence at each time step. This corresponds to the Context vector.\n\n*2)-Embedding Layer:*\n  * embedding.shape = (1, N, embedding_size)\n\n*3)-LSTM Layer:*\n  * output.shape = (1, N, hidden_size)\n  * hidden.shape = (num_layers, N, hidden_size)\n  * cell.shape = (num_layers, N, hidden_size)\n  \n*4)-Fully Connected Layer:*\n  * predictions.shape = (1, N, output_size), to send it to the loss function we want it to be (N, output_size) so we're just gonna remove the first dimension.\n  * predictions.shape (after squeeze) = (N,    output_size).\n  ","metadata":{}},{"cell_type":"code","source":"# Encoder Block LSTM\n\nclass EncoderSS2S(nn.Module):\n    \n    def __init__(self, input_size, embedding_size, hidden_size, num_layers, p):\n        super(EncoderSS2S, self).__init__()\n        \n        self.dropout = nn.Dropout(p)\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        \n        # Layers of the encoding block\n        self.embedding = nn.Embedding(input_size, embedding_size)\n        self.lstm = nn.LSTM(embedding_size, hidden_size, num_layers, dropout=p, batch_first=False)\n        \n    def forward(self, x):\n        \"\"\"\n        -> x.shape() = (seq_lenght, batch_size)\n        -> emb.shape() = (seq_lenght, batch_size, embedding_size)\n        -> out.shape() = (seq_lenght, batch_size, hidden_size)\n        -> hidden.shape() = (num_layers, batch_size, hidden_size)\n        -> cell.shape() = (num_layers, batch_size, hidden_size)\n        \"\"\"\n        embedding = self.dropout(self.embedding(x))\n        out, (hidden, cell) = self.lstm(embedding)\n        return hidden, cell\n    \n# Decoder Block LSTM\n\nclass DecoderSS2S(nn.Module):\n    \n    def __init__(self, input_size, embedding_size, hidden_size, output_size,num_layers, p):\n        super(DecoderSS2S, self).__init__()\n        \n        self.dropout = nn.Dropout(p)\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.output_size = output_size\n        self.num_layers = num_layers\n        \n        # Layers of the Decoder block\n        \n        self.embedding = nn.Embedding(input_size, embedding_size)\n        self.lstm = nn.LSTM(embedding_size, hidden_size, num_layers, dropout=p, batch_first=False)\n        self.fc = nn.Linear(hidden_size, output_size)\n    \n    def forward(self, x, hidden, cell):\n        \"\"\"\n        ->x.shape() = (batch_size, 1, embedding_size)\n        ->embedding.shape() = (batch_size, 1, embedding_size)\n        \"\"\"\n        x = x.unsqueeze(0)\n        embedding = self.dropout(self.embedding(x))\n        out, (hidden, cell) = self.lstm(embedding, (hidden,cell))\n        \n        predictions = self.fc(out)\n        \n        predictions = predictions.squeeze(0)\n        \n        return predictions, hidden, cell\n        ","metadata":{"execution":{"iopub.status.busy":"2023-07-26T05:11:24.010996Z","iopub.execute_input":"2023-07-26T05:11:24.011701Z","iopub.status.idle":"2023-07-26T05:11:24.024997Z","shell.execute_reply.started":"2023-07-26T05:11:24.011669Z","shell.execute_reply":"2023-07-26T05:11:24.024001Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In the forward functins of the decoder, the decoder expects a single word at each time step instead of an entire sequence. In a Seq2Seq model, the decoder processes one token at a time during inference, generating the output sequence word by word. However, in practice, we still need to handle batch processing for efficiency. To make if compatible with the LSTM, which expects a 3-dimensional input with dimensions (batch_size, seq_lenght, input_size), the ``x`` tensor with shape ``(N)`` (where ``N`` is the batch size) needs to be reshaped to ``(1,N)`` to represent a single word token. The additional dimension of 1 indicates that the input word is a sequence of lenght 1.\n\nOn the other hand, the ``predictions`` tensor represents the output probabilities for each word in the ``target`` vocabulary. Its shape is ``(1, N, tk_out_size)`` where ``1`` is the sequence lenght (we predict one word at a time), ``N`` is the batch size, and  \"tk_out_lenght\" is the lenght of the target vocabulary.\n\nHowever, for calculating the loss function during training, we need to compare the predictions with the actual targets. The loss function generally expects the predictions to have the shale ``(N, tk_out_size)`` to compare them with the target tensor of the same shape. To achive this, the DecoderSS2S class remove the first dimension of the  \"predictions\" tensor using  ``squeeze(0)``. This operation effectively converts the shape from \"``(1 , N, tk_out_size)`` to  ``(N, tk_out_size)``.","metadata":{}},{"cell_type":"markdown","source":"The ``teacher_force_ratio`` is a hyperparameter that controls how much teacher forcing is used during training.  Teacher forcing is a technique commonly used in sequence-to-sequence models to stabilize and speed up the training.\n\nWhen ``teacher_force_ratio`` is set to ``1.0``, the model uses teacher forcing for all time steps during trainign. Teacher forcing means that the decoder is feed with the actual ground truth  target words at each time step, rather than using its own predictions as input. This helps the model to learn more quicky and effectivelly, especially when the predicted outputs may be incorrect during early stages of training. On the other hand, when ``teacher_force_ratio`` is set to ``0.0`` the model does not use teacher forching at all, then, the decoderÂ´s predictions from the previous timestep are used as input to the decoder at the current time step. This forces the model to rely on its own redictions and can be useful for generating mode diverse outputs during inference.","metadata":{}},{"cell_type":"code","source":"class SSeq2Seq(nn.Module):\n    \n    def __init__(self, encoder, decoder, out_vocab):\n        super(SSeq2Seq, self).__init__()\n        \n        self.encoder = encoder\n        self.decoder = decoder\n        self.dec_out_vocab = out_vocab\n        \n    def forward(self, source, target, teacher_force_ratio=0.5):\n        batch_size = source.shape[1]\n        target_len = target.shape[0]\n        target_vocab_size = len(self.dec_out_vocab)\n        \n        output = torch.zeros(target_len, batch_size, target_vocab_size).to(device)\n        \n        hidden, cell = self.encoder(source)\n        \n        # Grab the first input to the Decoder which will be <start> token\n        x = target[0]\n        \n        output_predictions = []  # Store output predictions for each time step\n        \n        for t in range(1, target_len):\n            # Pick the previous hidden and cell tensors as context from encoder at start\n            output_t, hidden, cell = self.decoder(x, hidden, cell)\n            \n            # Store the next output prediction\n            #output[t] = output_t\n            \n            # Append the output prediction to the list\n            output_predictions.append(output_t)\n            \n            # Get the best word that the Decoder predict (index in the vocabulary)\n            best_guess = output_t.argmax(1)\n            \n            x = target[t] if random.random() < teacher_force_ratio else best_guess\n            \n        # Concatenate the list of output predictions to get the final output tensor\n        output = torch.stack(output_predictions, dim=0)\n        \n        return output\n             ","metadata":{"execution":{"iopub.status.busy":"2023-07-26T05:11:27.432817Z","iopub.execute_input":"2023-07-26T05:11:27.433842Z","iopub.status.idle":"2023-07-26T05:11:27.445974Z","shell.execute_reply.started":"2023-07-26T05:11:27.433793Z","shell.execute_reply":"2023-07-26T05:11:27.444763Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Model Hipper parameters\nload_model = False\nnum_layers = 3\nlearning_rate = 0.001\nnum_epochs = 10\n\n# x hipper parameters\n\ninput_size_encoder = len(tk_inp)\ninput_size_decoder = len(tk_out)\noutput_size = len(tk_out)\n\nencoder_embedding_size = 150\ndecoder_embedding_size = 150\nhidden_size = 256 # encoding units, same for each LSTM block\n\nbatch_size = 256\ninput_size = 35\nsequence_length = 35\noutput_size = 35\n\nenc_dropout = 0.5\ndec_dropout = 0.5\n\n# Tensorboard to get nice loss plot\nwriter = SummaryWriter(f\"runs/loss_plot\")\nstep = 0\n\n# Momentum\nbeta1 = 0.1  # Set your desired momentum value here (for the momentum term in Adam)\nbeta2 = 0.1  # Set your desired value here (for the squared gradient term in Adam)\n","metadata":{"execution":{"iopub.status.busy":"2023-07-26T05:11:30.089745Z","iopub.execute_input":"2023-07-26T05:11:30.090190Z","iopub.status.idle":"2023-07-26T05:11:30.100673Z","shell.execute_reply.started":"2023-07-26T05:11:30.090154Z","shell.execute_reply":"2023-07-26T05:11:30.099480Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Model Seq2Seq instance\n\n# ->Encoder network\nencoder_net = EncoderSS2S(\n    input_size_encoder, encoder_embedding_size, hidden_size, num_layers, enc_dropout\n).to(device)\n\n# ->Decoder network\ndecoder_net = DecoderSS2S(\n    input_size_decoder,\n    decoder_embedding_size,\n    hidden_size,\n    output_size,\n    num_layers,\n    dec_dropout,\n).to(device)\n\n# ->S2S Network\nmodel_SS2S = SSeq2Seq(encoder_net, decoder_net, tk_inp).to(device)\n","metadata":{"execution":{"iopub.status.busy":"2023-07-26T05:11:33.157710Z","iopub.execute_input":"2023-07-26T05:11:33.158142Z","iopub.status.idle":"2023-07-26T05:11:33.398017Z","shell.execute_reply.started":"2023-07-26T05:11:33.158109Z","shell.execute_reply":"2023-07-26T05:11:33.396877Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Loss and optimizer\npad_idx = tk_out.get_stoi()[\"<pad>\"]\ncriterionSS2S = nn.CrossEntropyLoss(ignore_index=pad_idx)\n\n\n#criterionSS2S = nn.CrossEntropyLoss()\noptimizerSS2S = torch.optim.Adam(model_SS2S.parameters(),\n                             lr = learning_rate\n                             #,betas=(beta1, beta2)\n                            )","metadata":{"execution":{"iopub.status.busy":"2023-07-26T05:11:44.454946Z","iopub.execute_input":"2023-07-26T05:11:44.455451Z","iopub.status.idle":"2023-07-26T05:11:44.516504Z","shell.execute_reply.started":"2023-07-26T05:11:44.455412Z","shell.execute_reply":"2023-07-26T05:11:44.515241Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 3.2.2.1-Training procedure <a class=\"anchor\" id=\"1-bullet\"></a> ","metadata":{}},{"cell_type":"code","source":"# Training procedure\n\nsentence = \"i was will be there\"\ntrain_DL = train_loader\n\nif load_model:\n    lead_checkpoint(torch.load(\"my_checkpoint.pth.tar\"), model, optimizer)\n\n    \nfor epoch in range(num_epochs):\n    print(f\"[Epoch {epoch} / {num_epochs}]\")\n    \n    checkpoint = {\n        \"state_dict\": model_SS2S.state_dict(),\n        \"optimizer\": optimizerSS2S.state_dict() \n                 }\n    save_checkpoint(checkpoint)\n    \n    model_SS2S.eval() #\n    \n    model_SS2S.train()\n    \n    for batch_idx, batch_data in enumerate(train_loader):\n        \n        # Get input and targets and move them to the device\n        enc_input, dec_input, dec_output = batch_data\n        enc_input, dec_input, dec_output = enc_input.to(device), dec_input.to(device), dec_output.to(device)\n        inputs, target = enc_input, dec_output\n        \n        # Forward prop\n        output = model_SS2S(inputs, targets)\n        # Transpose output tensor to match target's shape\n        output = output.permute(1, 0, 2)\n        \n        # Remove the first timestep from both output and target\n        output = output[1:].reshape(-1, output.shape[2])\n        target = target[1:].reshape(-1)\n        \n        optimizerSS2S.zero_grad()\n        loss = criterionSS2S(output, target)\n        \n        # Back prop\n        loss.backward()\n        \n        # Avoid exploding gradients, here, we make sure that gradients \n        # are within a stipulated range\n        \n        torch.nn.utils.clip_grad_norm(model_SS2S.parameters(), max_norm=1)\n        \n        # Gradient descent step\n        optimizer.step()\n        \n        # Plot to tensorboard\n        writer.add_scalar(\"Training loss\", loss, global_step=step)\n        step += 1\n        \nscore = bleu(test_data[1:100], model, german, english, device)\nprint(f\"Bleu score {score*100:.2f}\")","metadata":{"execution":{"iopub.status.busy":"2023-07-26T05:11:48.110903Z","iopub.execute_input":"2023-07-26T05:11:48.111509Z","iopub.status.idle":"2023-07-26T05:11:51.262468Z","shell.execute_reply.started":"2023-07-26T05:11:48.111475Z","shell.execute_reply":"2023-07-26T05:11:51.261207Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Training procedure\n\nsentence = \"i was will be there\"\n\nif load_model:\n    lead_checkpoint(torch.load(\"my_checkpoint.pth.tar\"), model, optimizer)\n\n    \nfor epoch in range(num_epochs):\n    print(f\"[Epoch {epoch} / {num_epochs}]\")\n    \n    checkpoint = {\n        \"state_dict\": model_SS2S.state_dict(),\n        \"optimizer\": optimizerSS2S.state_dict() \n                 }\n    save_checkpoint(checkpoint)\n    \n    model_SS2S.eval() #\n    \n    model_SS2S.train()\n    \n    for betch_idx, batch in enumerate(train_iterator):\n        \n        # Get input and targets and move them to the device\n        inp_data = batch.src.to(device)\n        target = batch.trg.to(device)\n        \n        # Forward prop\n        output = model_SS2S(inp_data, target)\n        \n        output = output[1:].reshape(-1, output.shape[2])\n        target = target[1:].reshape(-1)\n        \n        optimizer.zero_grad()\n        loss = criterion(output, target)\n        \n        # Back prop\n        loss.backward()\n        \n        # Avoid exploding gradients, here, we make sure that gradients \n        # are within a stipulated range\n        \n        torch.nn.utils.clip_grad_norm(model.parameters(), max_norm=1)\n        \n        # Gradient descent step\n        optimizer.step()\n        \n        # Plot to tensorboard\n        writer.add_scalar(\"Training loss\", loss, global_step=step)\n        step += 1\n        \nscore = bleu(test_data[1:100], model, german, english, device)\nprint(f\"Bleu score {score*100:.2f}\")","metadata":{"execution":{"iopub.status.busy":"2023-07-26T04:15:54.782676Z","iopub.execute_input":"2023-07-26T04:15:54.783166Z","iopub.status.idle":"2023-07-26T04:15:55.087522Z","shell.execute_reply.started":"2023-07-26T04:15:54.783118Z","shell.execute_reply":"2023-07-26T04:15:55.085985Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3.2.2-Seq2Seq LSTM with Attention model <a class=\"anchor\" id=\"1-bullet\"></a> ","metadata":{}},{"cell_type":"markdown","source":"#### 3.1-Encoder - Decoder Layers","metadata":{"id":"yYd__h7awBg2"}},{"cell_type":"code","source":"# Encoder Class\n\nclass Encoder(nn.Module):\n\n  def __init__(self, vocab_size, embedding_dim, enc_units, input_len):\n    super(Encoder, self).__init__()\n\n    self.vocab_size = vocab_size\n    self.embedding_dim = embedding_dim\n    self.enc_units = enc_units\n    self.input_len = input_len\n\n    # Embedding layer initialization\n    self.embedding = nn.Embedding(self.vocab_size, self.embedding_dim)\n\n    # Bidirectional LSTM layer initialization\n    self.lstm_bi = nn.LSTM(self.embedding_dim, self.enc_units, bidirectional=True, batch_first=True)\n\n  def forward(self, input):\n    # Convert input to embedded vectors\n    emb = self.embedding(input)\n\n    # Passing through the Bidirectional LSTM layer\n    enc_output, (state_h, state_c) = self.lstm_bi(emb)\n\n    return enc_output, state_h, state_c","metadata":{"id":"65k4Xq6hMGhD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Decoder Class\n\nclass Decoder(nn.Module):\n\n  def __init__(self, vocab_size, embedding_dim, dec_unit, input_len):\n    super(Decoder, self).__init__()\n    \n\n    self.vocab_size = vocab_size\n    self.embedding_dim = embedding_dim\n    self.dec_unit = dec_unit\n    self.input_len = input_len\n\n    # Embedding and LSTM layer initialization\n\n    self.embedding = nn.Embedding(self.vocab_size, self.embedding_dim)\n    self.lstm = nn.LSTM(self.embedding_dim, self.dec_unit, batch_first=True)\n\n  def forward(self, input, state):\n    # Embedded vectors\n    emb = self.embedding(input)\n    # LSTM output\n    dec_out, (state_h, state_c) = self.lstm(emb,state)\n\n    return dec_out, state_h, state_c\n","metadata":{"id":"Gc07oxEHy3SJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"####3.2-Model Architecture","metadata":{"id":"oxxkQmwS5H5N"}},{"cell_type":"code","source":"# Creating the model\n\nclass Seq2Seq(nn.Module):\n    def __init__(self, vocab_size_in, vocab_size_out, embedding_dim, enc_units, dec_units, input_len):\n        super(Seq2Seq, self).__init__()\n\n        self.vocab_size_in = vocab_size_in\n        self.vocab_size_out = vocab_size_out\n        self.embedding_dim = embedding_dim\n        self.enc_units = enc_units\n        self.dec_units = dec_units\n        self.input_len = input_len\n\n        # Initialize the Encoder and Decoder inside the forward pass\n        self.encoder = Encoder(self.vocab_size_in, self.embedding_dim, self.enc_units, self.input_len)\n        self.decoder = Decoder(self.vocab_size_out, self.embedding_dim, self.dec_units, self.input_len)\n\n        # Initializing the Dense Layer with Softmax activation\n        self.dense = nn.Linear(dec_units, vocab_size_out)\n        self.softmax = nn.Softmax(dim=2)\n\n    def forward(self, enc_input, dec_input):\n        # Getting the Encoder output and states\n        enc_output, enc_state_h, enc_state_c = self.encoder(enc_input)\n        print(enc_output.shape)\n        print(dec_input.shape)\n        # Storing the Encoder states in a variable\n        enc_state = [enc_state_h, enc_state_c]\n\n        # Getting the Decoder output and states\n        dec_output, _, _ = self.decoder(dec_input, enc_state)\n\n        # Applying Softmax activation to the dense layer\n        dense_output = self.softmax(self.dense(dec_output))\n\n        return dense_output\n\n# Model Initialization\nvocab_size_in = len(tk_inp)\nvocab_size_out = len(tk_out)\nembedding_dim = 300\nenc_units = 256\ndec_units = 512\ninput_len = 35\nmodel = Seq2Seq(vocab_size_in=vocab_size_in,\n                vocab_size_out=vocab_size_out,\n                embedding_dim=embedding_dim,\n                enc_units=enc_units,\n                dec_units=dec_units,\n                input_len=input_len)\n","metadata":{"id":"n6ZPGqddCpD9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for batch_idx, (enc_input, dec_input, target) in enumerate(train_loader):\n    # Print the shapes of input tensors\n    print(\"Encoder Input Shape:\", enc_input.shape)\n    print(\"Decoder Input Shape:\", dec_input.shape)\n    print(\"Target Shape:\", target.shape)\n\n    # Pass the data through the model\n    output = model(enc_input, dec_input)\n\n    # Print the shape of the output\n    print(\"Output Shape:\", output.shape)","metadata":{"id":"XDUbIXHz4JSF","executionInfo":{"status":"error","timestamp":1690086649940,"user_tz":300,"elapsed":1174,"user":{"displayName":"Nicolas Castillo Ojeda","userId":"07071882850745015721"}},"outputId":"8e2499a1-285c-492e-f18d-05f2425fe4a4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"####3.3-Training Procedure","metadata":{"id":"RpKJutGY5Os1"}},{"cell_type":"code","source":"# Training Callbacks of the model\n\nclass Train_Callback:\n\n  def __init__(self, model, train_dataloader, val_dataloader, checkpoint_path, log_path, patience=5, min_delta=0.0001):\n    self.model = model\n    self.train_dataloader = train_dataloader\n    self.val_dataloader = val_dataloader\n    self.checkpoint_path = checkpoint_path\n    self.log_path = log_path\n    self.patience = patience\n    self.min_delta = min_delta\n\n  def train(self, num_epochs, vocab_size_in, vocab_size_out, embedding_dim, enc_units, dec_units, input_len):\n\n    # TensorBoard writer for logging\n    writer = SummaryWriter(self.log_path)\n\n    # Training and validation steps for one epoch\n    train_steps = len(self.train_dataloader)\n    val_steps = len(self.val_dataloader)\n\n    # Loss function and optimizer\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(self.model.parameters())\n\n    # Initialization of early stoping variables\n    best_val_los = float('inf')\n    counter = 0\n\n    # Training loop\n    for epoch in range(num_epochs):\n      self.model.train()\n      total_loss = 0.0\n\n      for batch_idx, (enc_input, dec_input, target) in enumerate(self.train_dataloader):\n        # Zero the gradients\n        optimizer.zero_grad()\n        # Fordward propagation\n        output = self.model(enc_input, dec_input)\n        # Loss computation\n        loss = criterion(output.view(-1, output.size(-1)), target.view(-1))\n        # BackPropagation\n        loss.backward()\n        # Weight update\n        optimizer.step()\n        # Total loss accumulation\n        total_loss +=loss.item()\n\n      # Average training loss for the epoch\n      average_train_loss = total_loss/train_steps\n\n      # Validation loop\n      self.model.eval()\n      val_loss = 0.0\n\n      with torch.no_grad():\n        for batch_idx, (enc_input, dec_input, target) in enumerate(self.val_dataloader):\n           # Fordward propagation\n           output = self.model(enc_input, dec_input)\n           # Loss term\n           loss = criterion(output.view(-1, output.size(-1)), target.view(-1))\n           # Loss accumulation\n           val_loss += loss.item()\n        # Validation average loss\n        average_val_loss = val_loss / val_steps\n\n        # Early stoping\n        if average_val_loss - best_val_loss > self.min_delta:\n          counter += 1\n        else:\n          counter = 0\n\n        if counter >= self.patience:\n          print(\"Early stopping reached. Training finished.\")\n          break\n\n        # Save the best model\n        if average_val_loss < best_val_loss:\n          best_val_loss = average_val_loss\n          torch.save(self.model.state_dict(), self.checkpoint_path)\n\n        # Log the losses in TensorBoard\n        writer.add_scalar(\"Loss/train\", average_train_loss, epoch)\n        writer.add_scalar(\"Loss/validation\", average_val_loss, epoch)\n\n      # Close the TensorBoard writer\n      writer.close()\n\n\n","metadata":{"id":"zhcu8MsVfZR8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"checkpoint_path = \"/content/drive/MyDrive/Colab Notebooks/2)-Machine Learning/4)-NeuroMatch Academy -DL/NLP - Project/Models/bidirectional_train.pth\"\nlog_path = \"/content/drive/MyDrive/Colab Notebooks/2)-Machine Learning/4)-NeuroMatch Academy -DL/NLP - Project/Models/bidirectional_train.pth/logs\"\npatience = 5\n\nvocab_size_in = len(tk_inp)  # 70118\nvocab_size_out = len(tk_out)  # 55474\nembedding_dim = 300\nenc_units = 256\ndec_units = 512\ninput_len = 35\n\n# Trainer instance initializatin\ntrainer = Train_Callback(model, train_loader, val_loader, checkpoint_path, log_path)\n# Train the model\nnum_epochs = 10\ntrainer.train(num_epochs=num_epochs,\n              vocab_size_in=vocab_size_in,\n              vocab_size_out=vocab_size_out,\n              embedding_dim=embedding_dim,\n              enc_units=enc_units,\n              dec_units=dec_units,\n              input_len=input_len)","metadata":{"id":"ILEtmV7wgtWM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"encoder = Encoder(vocab_size_in, embedding_dim, enc_units, input_len)\n\n# Create some example data for the encoder input\n# Note: The input should be a tensor with shape (batch_size, input_len)\n# Here, we are assuming batch_size=2 for demonstration purposes\nbatch_size = 512\nexample_input = torch.randint(0, vocab_size_in, size=(batch_size, input_len))\n\n# Pass the input through the encoder\nenc_output, state_h, state_c = encoder(example_input)\n\n# Print the shapes of the encoder output and states\nprint(\"Encoder Output Shape:\", enc_output.shape)\nprint(\"State_h Shape:\", state_h.shape)\nprint(\"State_c Shape:\", state_c.shape)","metadata":{"id":"b2h0BvzkFR6-","executionInfo":{"status":"ok","timestamp":1690068875369,"user_tz":300,"elapsed":1778,"user":{"displayName":"Nicolas Castillo Ojeda","userId":"07071882850745015721"}},"outputId":"3a930f7f-fd87-45a5-f260-c5c11c304bc7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for batch_idx, (enc_input, dec_input, target) in enumerate(train_loader):\n    # Pass the data through the model\n    output = model(enc_input, dec_input)\n\n    # Print the shape of the output\n    print(\"Output Shape:\", output.shape)","metadata":{"id":"ptUUMAH817aN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"max_index_inp = max(tk_inp.get_stoi().values())\nmax_index_out = max(tk_out.get_stoi().values())\nprint(\"Maximum token index (encoder input):\", max_index_inp)\nprint(\"Maximum token index (decoder input):\", max_index_out)","metadata":{"id":"_JgTzWJk4gel","executionInfo":{"status":"ok","timestamp":1690068891866,"user_tz":300,"elapsed":357,"user":{"displayName":"Nicolas Castillo Ojeda","userId":"07071882850745015721"}},"outputId":"f9a5074a-83cf-4352-eefb-e751d1e5bad8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"max_index_enc_input = torch.max(enc_input)\nmax_index_dec_input = torch.max(dec_input)\nmax_index_target = torch.max(target)\n\nprint(\"Max Index Encoder Input:\", max_index_enc_input)\nprint(\"Max Index Decoder Input:\", max_index_dec_input)\nprint(\"Max Index Target:\", max_index_target)","metadata":{"id":"U9I9KskI4sc2","executionInfo":{"status":"ok","timestamp":1690068888452,"user_tz":300,"elapsed":221,"user":{"displayName":"Nicolas Castillo Ojeda","userId":"07071882850745015721"}},"outputId":"e7828793-f90c-4cae-cb1c-3a86758bd0ef"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#3.1-Grammar error detection (inference)","metadata":{"id":"7hzD3KitlPwe"}},{"cell_type":"code","source":"","metadata":{"id":"aFfYT631kr7W"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##3.2-Grammar error correction\n","metadata":{"id":"ULt_kewylSdO"}},{"cell_type":"markdown","source":"#4)-Training","metadata":{"id":"rNMdM1YqkslW"}},{"cell_type":"code","source":"","metadata":{"id":"G7peiPrRkw2F"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#5)-Evaluation","metadata":{"id":"EOjdvc0Kkxmm"}},{"cell_type":"code","source":"","metadata":{"id":"9dTJubnWkzz2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#6)-Inference\n","metadata":{"id":"hKKNwKMYk2AN"}},{"cell_type":"code","source":"","metadata":{"id":"4c3XizExk6Sl"},"execution_count":null,"outputs":[]}]}